import streamlit as st
import time

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama

# =========================
# CONFIGURACI√ìN GENERAL
# =========================

PDF_PATH = "documento.pdf"   # üëà aseg√∫rate que exista
LLM_MODEL = "llama3"
EMBED_MODEL = "nomic-embed-text"
CHROMA_DIR = "chroma_db"

# =========================
# STREAMLIT UI
# =========================

st.set_page_config(
    page_title="Asistente Investigador",
    page_icon="üß†",
    layout="centered"
)

st.title("üß† Asistente Investigador")
st.caption("Respuestas en tiempo real ¬∑ Modo an√°lisis activado")

# =========================
# CARGA DEL SISTEMA (UNA VEZ)
# =========================

def cargar_sistema():
    with st.spinner("üìÑ Cargando documento..."):
        loader = PyPDFLoader(PDF_PATH)
        docs = loader.load()
        st.success(f"Documento cargado: {len(docs)} p√°ginas")

    with st.spinner("‚úÇÔ∏è Fragmentando conocimiento..."):
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150
        )
        texts = splitter.split_documents(docs)
        st.success(f"Fragmentos creados: {len(texts)}")

    with st.spinner("üß† Inicializando embeddings (local)..."):
        embeddings = OllamaEmbeddings(model=EMBED_MODEL)

    with st.spinner("üì¶ Construyendo memoria vectorial..."):
        db = Chroma.from_documents(
            texts,
            embedding=embeddings,
            persist_directory=CHROMA_DIR
        )

    with st.spinner("ü§ñ Despertando el modelo cognitivo..."):
        llm = Ollama(
            model=LLM_MODEL,
            temperature=0.2
        )

        qa = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=db.as_retriever(),
            return_source_documents=False
        )

    return qa

# =========================
# INICIALIZACI√ìN CONTROLADA
# =========================

if "qa" not in st.session_state:
    st.info("Inicializando sistema por primera vez‚Ä¶")
    st.session_state.qa = cargar_sistema()
    st.success("Sistema listo. Puedes preguntar.")

qa_chain = st.session_state.qa

# =========================
# INTERACCI√ìN CON EL USUARIO
# =========================

st.divider()

pregunta = st.text_input(
    "üß† Haz tu pregunta:",
    placeholder="Ej. ¬øCu√°l es la idea principal del documento?"
)

if pregunta:
    with st.spinner("‚úçÔ∏è Pensando‚Ä¶"):
        inicio = time.time()
        respuesta = qa_chain.invoke(pregunta)
        fin = time.time()

    st.markdown("### üìù Respuesta")
    st.write(respuesta["result"])

    st.caption(f"‚è±Ô∏è Tiempo de respuesta: {fin - inicio:.2f} segundos")
